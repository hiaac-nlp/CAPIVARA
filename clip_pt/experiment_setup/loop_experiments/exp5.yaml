title: ${trainer.max_steps}-CC3M_PEFT_(bs-${batch_size})(LR-${optimizer.params.learning_rate})(proj-layer:${model.projection_layer})

datasets:
    train:
        - name: CC3M
          path: "/hadatasets/clip_pt/final_webdatasets/cc3m/{00000..00266}.tar"

    validation:
        - name: flickr30k
          path: "/hadatasets/clip_pt/final_webdatasets/flickr30k_val/00000.tar"

    img_classification:
        path: "/hadatasets/clip_pt/validation_datasets/GroceryStoreDataset/dataset/val.txt"
        annotation_path: "/hadatasets/clip_pt/validation_datasets/GroceryStoreDataset/dataset/translated_classes_gsd.csv"

model:
    adapter: "LoRA"
    number_layers: 11
    progressive_adapter: -1
    projection_layer: 'true'

optimizer:
    name: "Adam"
    params:
        learning_rate: 1e-5
        min_learning_rate: 1e-6
        eps: 1e-6
        weight_decay: 0.2
        betas: [ 0.9, 0.98 ]

scheduler:
    name: "CosineWarmupLR"
    params:
        warmup_lr: 250


batch_size: 2816
accumulate_grad_batches: 1
tags:
    - Adapters
    - ${trainer.max_steps}-Steps
    - Filtered-Dataset
    - ${batch_size}-BS
    - LR-${optimizer.params.learning_rate}
    - proj-layer-${model.projection_layer}
    
trainer:
    accelerator: "gpu"
    max_steps: 5863  #
    num_sanity_val_steps: 2
    overfit_batches: 0.0
    log_every_n_steps: 10
    val_check_interval: 100

model_checkpoint:
    mode: "min"
    save_last: true
    save_weights_only: true
    monitor: "val/classification_loss/dataloader_idx_1"
    dirpath: "../checkpoints/clip_pt/MSCOCO_LiT"

early_stopping:
    verbose: true
    monitor: "val/classification_loss/dataloader_idx_1"
    patience: 100

carbon:
    carbon_checker: "false"
    brazil_carbon_intensity: 0.077
    process: "process"
