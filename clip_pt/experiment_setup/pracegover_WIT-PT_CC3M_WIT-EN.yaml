title: datasets_without_translation

datasets:
    train:
        - name: pracegover
          path: "/hadatasets/clip_pt/final_webdatasets/pracegover_train/{00000..00024}.tar"
        - name: WIT-PT
          path: "/hadatasets/clip_pt/final_webdatasets/WIT_PT_TRAIN/{00000..00017}.tar"
        - name: CC3M
          path: "/hadatasets/clip_pt/final_webdatasets/cc3m-with-repetition/{00000..00266}.tar"
        - name: WIT-EN
          path: "/hadatasets/clip_pt/final_webdatasets/WIT_EN_TRAIN/{00000..000170}.tar"

    validation:
        - name: pracegover
          path: "/hadatasets/clip_pt/final_webdatasets/pracegover_val/{00000..00008}.tar"

model:
    text_encoder: "neuralmind/bert-base-portuguese-cased"
    image_encoder: "openai/clip-vit-base-patch32"
    text_padding_size: 95
    warmup_steps: 1000
    pretraining: "LiT" #options: LiT / contrastive

optimizer:
    name: "Adam"
    params:
        learning_rate: 5e-5
        eps: 1e-6
        weight_decay: 0.001
        betas: [ 0.9, 0.98 ]

scheduler:
    name: "CosineWarmupLR"
    params:
        warmup_lr: 800


batch_size: 2816
accumulate_grad_batches: 1

trainer:
    accelerator: "gpu"
    max_epochs: 20  #
    num_sanity_val_steps: 2
    overfit_batches: 0.0
    log_every_n_steps: 10

model_checkpoint:
    mode: "min"
    save_last: true
    save_weights_only: true
    monitor: "val/loss"
    dirpath: "../checkpoints/clip_pt/datasets_without_translation"

early_stopping:
    verbose: true
    monitor: "val/loss"
    patience: 100

